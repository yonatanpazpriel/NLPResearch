{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "b3599d30-639d-4a98-a5f8-9d060132b526",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Load the CSV file\n",
    "text_1_path = \"csv files/text_1.csv\"  \n",
    "text_2_path = \"csv files/text_2.csv\"  \n",
    "text_3_path = \"csv files/text_3.csv\"  \n",
    "text_4_path = \"csv files/text_4.csv\"  \n",
    "text_5_path = \"csv files/text_5.csv\"  \n",
    "text_6_path = \"csv files/text_6.csv\"  \n",
    "\n",
    "df1, df2, df3 = pd.read_csv(text_1_path), pd.read_csv(text_2_path), pd.read_csv(text_3_path)\n",
    "df4, df5, df6 = pd.read_csv(text_4_path), pd.read_csv(text_5_path), pd.read_csv(text_6_path)\n",
    "\n",
    "data = pd.concat([df1, df2, df3, df4, df5, df6], ignore_index=True)\n",
    "\n",
    "# Ensure the necessary column exists\n",
    "if \"preprocessed_text\" not in data.columns:\n",
    "    raise ValueError(\"The column 'preprocessed_text' is missing from the CSV file.\")\n",
    "    \n",
    "vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = vectorizer.fit_transform(data['preprocessed_text'])\n",
    "\n",
    "# Compute cosine similarity matrix\n",
    "similarity_matrix = cosine_similarity(tfidf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "900550ad-5bb9-40c1-a2a3-ab08d171d148",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 449 files with a duplicate. There are 6332 unique files.\n",
      "\n",
      "There are 10807 pairs of files with similarity above 0.95 and below 1 out of 6781 total files:\n",
      "\n",
      "8-11-20 Patterson_embedded.txt and 9-30-20 Patterson_embedded.txt - Similarity: 1.00\n",
      "8-13-20 Schmitt-Chan_embedded.txt and 8-12-20 Fearn_embedded.txt - Similarity: 0.96\n",
      "8-24-20 Parenteau_embedded.txt and 9-1-20 Cohen David_embedded.txt - Similarity: 0.98\n",
      "8-12-20 Gurin_embedded.txt and 9-6-20 Gurin_embedded.txt - Similarity: 1.00\n",
      "8-13-20 Loi-On Attachment 2_embedded.txt and 9-30-20 Loi-On Attachment 3_embedded.txt - Similarity: 0.99\n",
      "8-12-20 Niku_embedded.txt and 8-11-20 Niku_embedded.txt - Similarity: 0.99\n",
      "8-11-20 Lynn Attachment_embedded.txt and 8-10-20 Lynn Attachment_embedded.txt - Similarity: 1.00\n",
      "8-12-20 Fearn_embedded.txt and 8-12-20 Jamakatt_embedded.txt - Similarity: 0.97\n",
      "8-12-20 Fearn_embedded.txt and 8-11-20 Fearn_embedded.txt - Similarity: 0.97\n",
      "8-10-20 Locker_embedded.txt and 8-13-19 Wolman_embedded.txt - Similarity: 0.95\n"
     ]
    }
   ],
   "source": [
    "thresholdLowerBound = 0.95  # Adjust this as needed\n",
    "thresholdUpperBound = 1 # Adjust this as needed\n",
    "\n",
    "similar_pairs = [] # store file pairs with similarity above the threshold\n",
    "\n",
    "# Mask similarity matrix to find pairs above threshold\n",
    "mask = (similarity_matrix > thresholdLowerBound) & (similarity_matrix < thresholdUpperBound)\n",
    "indices = np.argwhere(mask)\n",
    "fileNames = data[\"file_name\"].to_numpy()\n",
    "\n",
    "# Collect file names for pairs\n",
    "similar_pairs = [(fileNames[i], fileNames[j], similarity_matrix[i, j]) \n",
    "                 for i, j in indices if i < j]  # Only upper triangle, since similarityMatrix is symmetrical (i.e. sm[i,j] = sm[j,i])\n",
    "\n",
    "duplicate_files = set(file1 for file1, file2, _ in similar_pairs).union(file2 for file1, file2, _ in similar_pairs)\n",
    "\n",
    "print(f\"There are {len(duplicate_files)} files with a duplicate. There are {len(fileNames) -len(duplicate_files)} unique files.\\n\")\n",
    "\n",
    "# Display the results\n",
    "print(f\"There are {len(similar_pairs)} pairs of files with similarity above {thresholdLowerBound} and below {thresholdUpperBound} out of {num_files} total files:\\n\")\n",
    "for file1, file2, score in similar_pairs[:10]:\n",
    "    print(f\"{file1} and {file2} - Similarity: {score:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1d6e999b-0609-4f5d-b260-a8ea20bf9059",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5793"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "files_to_remove = set()\n",
    "for i in range(num_files):\n",
    "    if i in files_to_remove:\n",
    "        continue  # Skip already marked files\n",
    "    for j in range(i + 1, num_files):\n",
    "        if j in files_to_remove:\n",
    "            continue  # Skip already marked files\n",
    "        if similarity_matrix[i, j] > 0.85:\n",
    "            # Mark the second file (j) as a duplicate\n",
    "            files_to_remove.add(j)\n",
    "\n",
    "# Create a DataFrame of unique files\n",
    "unique_files = data.drop(index=list(files_to_remove)).reset_index(drop=True)\n",
    "len(unique_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c749566-7cd1-4000-a1f8-10e43a260ccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import os\n",
    "\n",
    "# Set your OpenAI API key\n",
    "openai.api_key = \"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\"\n",
    "\n",
    "def send_to_openai(file_content):\n",
    "    \"\"\"\n",
    "    Send the file content to the OpenAI API to extract the number and sentence.\n",
    "    \"\"\"\n",
    "    system_prompt = (\n",
    "        \"You are a helpful assistant tasked with analyzing text files. \"\n",
    "        \"Each file contains comments submitted by the public. Some files \"\n",
    "        \"include a header that specifies how many people the file represents, \"\n",
    "        \"in terms of 'submissions', 'copies', or similar words. Your task is to:\\n\"\n",
    "        \"1. Identify and extract the number of people represented based on the text in the header.\\n\"\n",
    "        \"2. Return the full sentence from the file that contains this number.\\n\"\n",
    "        \"If no such information is found, return 1 as the default number and note that no header was identified.\"\n",
    "    )\n",
    "    \n",
    "    prompt = (\n",
    "        f\"Here is the content of the file:\\n\\n{file_content}\\n\\n\"\n",
    "        \"Please extract the number of people represented and the corresponding sentence.\"\n",
    "    )\n",
    "    \n",
    "    try:\n",
    "        response = openai.ChatCompletion.create(\n",
    "            model=\"gpt-4\",  # Use \"gpt-3.5-turbo\" if GPT-4 is not available\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": system_prompt},\n",
    "                {\"role\": \"user\", \"content\": prompt},\n",
    "            ]\n",
    "        )\n",
    "        return response['choices'][0]['message']['content']\n",
    "    except Exception as e:\n",
    "        print(f\"Error with OpenAI API: {e}\")\n",
    "        return None\n",
    "\n",
    "def process_files(file_list):\n",
    "    \"\"\"\n",
    "    Process a list of text files and send them to OpenAI.\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    for file_path in file_list:\n",
    "        print(f\"Processing {file_path}...\")\n",
    "        try:\n",
    "            with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                content = file.read()\n",
    "                result = send_to_openai(content)\n",
    "                if result:\n",
    "                    results.append({\"file\": file_path, \"result\": result})\n",
    "        except Exception as e:\n",
    "            print(f\"Error reading file {file_path}: {e}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "def save_results_to_file(results, output_path):\n",
    "    \"\"\"\n",
    "    Save the API results to a file for further analysis.\n",
    "    \"\"\"\n",
    "    with open(output_path, 'w', encoding='utf-8') as output_file:\n",
    "        for entry in results:\n",
    "            output_file.write(f\"File: {entry['file']}\\n\")\n",
    "            output_file.write(f\"Result: {entry['result']}\\n\")\n",
    "            output_file.write(\"-\" * 80 + \"\\n\")\n",
    "\n",
    "# Replace with the directory containing your .txt files\n",
    "directory = \"path_to_your_text_files\"\n",
    "file_list = [os.path.join(directory, f) for f in os.listdir(directory) if f.endswith(\".txt\")]\n",
    "\n",
    "# Process files and save results\n",
    "results = process_files(file_list)\n",
    "save_results_to_file(results, \"output_results.txt\")\n",
    "\n",
    "print(\"Processing complete. Results saved to output_results.txt.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
